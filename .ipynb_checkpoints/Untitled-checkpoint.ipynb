{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de2b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b22190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1ae880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/cv-other-train/sample-069205.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/cv-valid-train/sample-063134.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/cv-other-train/sample-080873.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/cv-other-train/sample-105595.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/cv-valid-train/sample-144613.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  gender\n",
       "0  data/cv-other-train/sample-069205.npy  female\n",
       "1  data/cv-valid-train/sample-063134.npy  female\n",
       "2  data/cv-other-train/sample-080873.npy  female\n",
       "3  data/cv-other-train/sample-105595.npy  female\n",
       "4  data/cv-valid-train/sample-144613.npy  female"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"balanced-all.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee13a620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66933</th>\n",
       "      <td>data/cv-valid-train/sample-171098.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66934</th>\n",
       "      <td>data/cv-other-train/sample-022864.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66935</th>\n",
       "      <td>data/cv-valid-train/sample-080933.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66936</th>\n",
       "      <td>data/cv-other-train/sample-012026.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66937</th>\n",
       "      <td>data/cv-other-train/sample-013841.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename gender\n",
       "66933  data/cv-valid-train/sample-171098.npy   male\n",
       "66934  data/cv-other-train/sample-022864.npy   male\n",
       "66935  data/cv-valid-train/sample-080933.npy   male\n",
       "66936  data/cv-other-train/sample-012026.npy   male\n",
       "66937  data/cv-other-train/sample-013841.npy   male"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample=len(df)\n",
    "n_male_sample=len(df[df['gender']=='male'])\n",
    "n_female_sample=len(df[df['gender']=='female'])\n",
    "print(\"total samples :\",n_sample)\n",
    "print(\"total male sample:\",n_male_sample)\n",
    "print(\"total female sample:\",n_female_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25614a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b752627",
   "metadata": {},
   "outputs": [],
   "source": [
    "lablel2int={\n",
    "    \"male\":1,\n",
    "    \"female\":0\n",
    "}\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    if os.path.isfile(\"results/features.npy\") and os.path.isfile(\"results/lables.npy\"):\n",
    "        X=np.load(\"results/features.npy\")\n",
    "        y=np.load(\"results/labels.npy\")\n",
    "        return X,y\n",
    "    df=pd.read_csv(\"balanced-all.csv\")\n",
    "    n_sample=len(df)\n",
    "    n_male_sample=len(df[df['gender']=='male'])\n",
    "    n_female_sample=len(df[df['gender']=='female'])\n",
    "    print(\"total samples :\",n_sample)\n",
    "    print(\"total male sample:\",n_male_sample)\n",
    "    print(\"total female sample:\",n_female_sample)\n",
    "    \n",
    "    X=np.zeros((n_sample, vector_length))\n",
    "    y=np.zeros((n_sample,1))\n",
    "    \n",
    "    for i,(filename,gender) in tqdm.tqdm(enumerate(zip(df['filename'],df['gender'])),\"Loding data\",total=n_sample):\n",
    "        features=np.load(filename)\n",
    "        X[i]=features\n",
    "        y[i]=lablel2int[gender]\n",
    "        \n",
    "    np.save(\"results/features\",X)\n",
    "    np.save(\"results/labels\",y)\n",
    "    return X,y\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67cd2f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 66938\n",
      "total male sample: 33469\n",
      "total female sample: 33469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loding data: 100%|███████████████████████████████████████████████████████████████| 66938/66938 [11:58<00:00, 93.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[8.33691835e-01, 1.84069288e+00, 7.21150815e-01, ...,\n",
       "         2.22568936e-03, 3.72419163e-04, 4.14350980e-05],\n",
       "        [3.44792323e-04, 3.78249679e-04, 7.02428690e-04, ...,\n",
       "         5.25400345e-08, 2.83253740e-08, 2.14376583e-09],\n",
       "        [1.90136401e-04, 2.43997492e-04, 4.60427953e-04, ...,\n",
       "         1.14822782e-04, 2.32082821e-05, 1.38619748e-06],\n",
       "        ...,\n",
       "        [1.16413343e-03, 9.43325169e-04, 1.52071775e-03, ...,\n",
       "         3.66348862e-09, 1.36717937e-09, 1.47780455e-10],\n",
       "        [6.36986643e-02, 1.34973586e-01, 2.42811322e+00, ...,\n",
       "         3.79042875e-04, 8.72957244e-05, 6.89193166e-06],\n",
       "        [1.45035911e+00, 1.17254317e+00, 2.72492021e-01, ...,\n",
       "         2.80375849e-03, 1.14007434e-03, 1.97923204e-04]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "276bc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c91ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y,test_size=0.1,valid_size=0.1):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_size,random_state=7)\n",
    "    X_train,X_valid,y_train,y_valid=train_test_split(X_train,y_train,test_size=valid_size,random_state=7)\n",
    "    return{\n",
    "        \"X_train\":X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb6129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 66938\n",
      "total male sample: 33469\n",
      "total female sample: 33469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loding data: 100%|███████████████████████████████████████████████████████████████| 66938/66938 [12:29<00:00, 89.30it/s]\n"
     ]
    }
   ],
   "source": [
    "X,y=load_data()\n",
    "data=split_data(X,y,test_size=0.1,valid_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e2ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b608b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vector_length=128):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(256,input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",metrics=[\"accuracy\"],optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598f7f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 205825 (804.00 KB)\n",
      "Trainable params: 205825 (804.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1513d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "666fb4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "848/848 [==============================] - 10s 8ms/step - loss: 0.5594 - accuracy: 0.7658 - val_loss: 0.3900 - val_accuracy: 0.8468\n",
      "Epoch 2/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.4144 - accuracy: 0.8376 - val_loss: 0.3281 - val_accuracy: 0.8689\n",
      "Epoch 3/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3789 - accuracy: 0.8548 - val_loss: 0.3247 - val_accuracy: 0.8745\n",
      "Epoch 4/100\n",
      "848/848 [==============================] - 7s 9ms/step - loss: 0.3578 - accuracy: 0.8612 - val_loss: 0.2998 - val_accuracy: 0.8838\n",
      "Epoch 5/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3478 - accuracy: 0.8678 - val_loss: 0.2889 - val_accuracy: 0.8861\n",
      "Epoch 6/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3291 - accuracy: 0.8742 - val_loss: 0.2857 - val_accuracy: 0.8901\n",
      "Epoch 7/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3296 - accuracy: 0.8749 - val_loss: 0.2849 - val_accuracy: 0.8855\n",
      "Epoch 8/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3192 - accuracy: 0.8797 - val_loss: 0.2809 - val_accuracy: 0.8969\n",
      "Epoch 9/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3090 - accuracy: 0.8830 - val_loss: 0.2700 - val_accuracy: 0.8939\n",
      "Epoch 10/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3070 - accuracy: 0.8848 - val_loss: 0.2643 - val_accuracy: 0.8966\n",
      "Epoch 11/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3012 - accuracy: 0.8864 - val_loss: 0.2573 - val_accuracy: 0.9002\n",
      "Epoch 12/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2976 - accuracy: 0.8891 - val_loss: 0.2633 - val_accuracy: 0.9002\n",
      "Epoch 13/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2926 - accuracy: 0.8899 - val_loss: 0.2555 - val_accuracy: 0.9046\n",
      "Epoch 14/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2937 - accuracy: 0.8899 - val_loss: 0.2494 - val_accuracy: 0.9097\n",
      "Epoch 15/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2827 - accuracy: 0.8934 - val_loss: 0.2488 - val_accuracy: 0.9089\n",
      "Epoch 16/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2884 - accuracy: 0.8914 - val_loss: 0.2504 - val_accuracy: 0.9052\n",
      "Epoch 17/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2792 - accuracy: 0.8956 - val_loss: 0.2517 - val_accuracy: 0.9094\n",
      "Epoch 18/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2794 - accuracy: 0.8973 - val_loss: 0.2637 - val_accuracy: 0.8974\n",
      "Epoch 19/100\n",
      "848/848 [==============================] - 6s 8ms/step - loss: 0.2736 - accuracy: 0.8976 - val_loss: 0.2395 - val_accuracy: 0.9115\n",
      "Epoch 20/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2767 - accuracy: 0.8978 - val_loss: 0.2466 - val_accuracy: 0.9085\n",
      "Epoch 21/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2765 - accuracy: 0.8985 - val_loss: 0.2428 - val_accuracy: 0.9067\n",
      "Epoch 22/100\n",
      "848/848 [==============================] - 7s 9ms/step - loss: 0.2771 - accuracy: 0.8994 - val_loss: 0.2315 - val_accuracy: 0.9149\n",
      "Epoch 23/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2714 - accuracy: 0.8994 - val_loss: 0.2386 - val_accuracy: 0.9100\n",
      "Epoch 24/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2734 - accuracy: 0.8979 - val_loss: 0.2364 - val_accuracy: 0.9105\n",
      "Epoch 25/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2698 - accuracy: 0.9004 - val_loss: 0.2432 - val_accuracy: 0.9059\n",
      "Epoch 26/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2712 - accuracy: 0.9007 - val_loss: 0.2350 - val_accuracy: 0.9150\n",
      "Epoch 27/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2656 - accuracy: 0.9015 - val_loss: 0.2351 - val_accuracy: 0.9140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13d81457a00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir=\"logs\")\n",
    "early_stopping=EarlyStopping(mode=\"min\",patience=5,restore_best_weights=True)\n",
    "\n",
    "batch_size=64\n",
    "epochs=100\n",
    "\n",
    "model.fit(data[\"X_train\"],data[\"y_train\"],epochs=epochs,batch_size=batch_size,validation_data=(data[\"X_valid\"],data[\"y_valid\"]),\n",
    "         callbacks=[tensorboard,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e71c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riya\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model in the native Keras format\n",
    "model.save(\"results/model.keras\")\n",
    "model.save(\"results/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d6d1478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model using 6694 samples...\n",
      "210/210 [==============================] - 1s 3ms/step - loss: 0.2327 - accuracy: 0.9135\n",
      "Loss:0.2327\n",
      "Accuracy: 91.35%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss,accuracy=model.evaluate(data[\"X_test\"],data[\"y_test\"],verbose=1)\n",
    "\n",
    "print(f\"Loss:{loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26242cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librosa is installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import librosa\n",
    "    print(\"librosa is installed.\")\n",
    "except ImportError:\n",
    "    print(\"librosa is not installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9af65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    \n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95eda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f36be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Speak into the microphone...\n",
      "Recording completed.\n",
      "Saved recorded voice to 'recorded_voice.wav'.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 5\n",
    "OUTPUT_FILE = \"recorded_voice.wav\"\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open the audio stream\n",
    "stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Recording started. Speak into the microphone...\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "# Record audio for the specified duration\n",
    "for i in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"Recording completed.\")\n",
    "\n",
    "# Stop and close the audio stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Save the recorded audio to a WAV file\n",
    "with wave.open(OUTPUT_FILE, \"wb\") as wf:\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "print(f\"Saved recorded voice to '{OUTPUT_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5292c3a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24388/2638365918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C:/Users/Riya/OneDrive/Desktop/voice detection/gender-recognition-by-voice/recorded_voice.wav\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmale_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfemale_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmale_prob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgender\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"male\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmale_prob\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mfemale_prob\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"female\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_feature' is not defined"
     ]
    }
   ],
   "source": [
    "file_name=\"C:/Users/Riya/OneDrive/Desktop/voice detection/gender-recognition-by-voice/recorded_voice.wav\"\n",
    "features = extract_feature(file_name, mel=True).reshape(1, -1)\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "\n",
    "# show the result!\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities:     Male: {male_prob * 100:.2f}%    Female: {female_prob * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cde8891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Speak into the microphone...\n",
      "Recording completed.\n",
      "Saved recorded voice to 'recorded_voice.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\Riya\\AppData\\Local\\Temp/ipykernel_20276/3471296818.py\", line 70, in predict_gender\n",
      "    features = extract_feature(file_name, mel=True).reshape(1, -1)\n",
      "NameError: name 'extract_feature' is not defined\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\Riya\\AppData\\Local\\Temp/ipykernel_20276/3471296818.py\", line 70, in predict_gender\n",
      "    features = extract_feature(file_name, mel=True).reshape(1, -1)\n",
      "NameError: name 'extract_feature' is not defined\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "from PIL import ImageTk,Image\n",
    "root=Tk()\n",
    "\n",
    "\n",
    "root.title(\"Voice-based gender detection\")\n",
    "root.geometry(\"600x400\")\n",
    "root.minsize(300,300)\n",
    "root.maxsize(700,700)\n",
    "\n",
    "root.iconbitmap(\"D:\\\\tkinter\\\\logo.ico\")\n",
    "root.configure(bg=\"lightblue\")\n",
    "\n",
    "header=Label(root,text=\"Voice based gender identification\",bg=\"lightblue\",\n",
    "            foreground=\"black\",font=(\"Arial\",24,\"bold\"))\n",
    "header.pack(pady=10)\n",
    "# root.wm_attributes('-transparentcolor',\"purple\")\n",
    "\n",
    "def record():\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    RECORD_SECONDS = 5\n",
    "    OUTPUT_FILE = \"recorded_voice.wav\"\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open the audio stream\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording started. Speak into the microphone...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    # Record audio for the specified duration\n",
    "    for i in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording completed.\")\n",
    "\n",
    "    # Stop and close the audio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded audio to a WAV file\n",
    "    with wave.open(OUTPUT_FILE, \"wb\") as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "    print(f\"Saved recorded voice to '{OUTPUT_FILE}'.\")\n",
    "button1=Button(text=\"Record voice\",bg=\"lightblue\",activebackground=\"blue\",font=(\"Arial\",18,\"bold\"),\n",
    "      borderwidth=3,command=record)\n",
    "button1.pack(pady=30)\n",
    "predict_label=Label(root,text=\"\",bg=\"lightblue\",font=(\"Arial\",15,\"bold\"))\n",
    "predict_label.pack()\n",
    "\n",
    "def predict_gender():\n",
    "    file_name=\"C:/Users/Riya/OneDrive/Desktop/voice detection/gender-recognition-by-voice/recorded_voice.wav\"\n",
    "    features = extract_feature(file_name, mel=True).reshape(1, -1)\n",
    "    male_prob = model.predict(features)[0][0]\n",
    "    female_prob = 1 - male_prob\n",
    "    gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "\n",
    "    # show the result!\n",
    "    print(\"Result:\", gender)\n",
    "    print(f\"Probabilities:     Male: {male_prob * 100:.2f}%    Female: {female_prob * 100:.2f}%\")\n",
    "    predict_label.config(text=\"Predicted Gender :\"+gender)\n",
    "\n",
    "button2=Button(text=\"Gender\",bg=\"lightblue\",activebackground=\"blue\",font=(\"Arial\",18,\"bold\"),\n",
    "      borderwidth=3,command=predict_gender)\n",
    "button2.pack(pady=25)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2be516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02ff66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645795bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
