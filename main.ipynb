{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de2b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b22190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d1ae880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/cv-other-train/sample-069205.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/cv-valid-train/sample-063134.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/cv-other-train/sample-080873.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/cv-other-train/sample-105595.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/cv-valid-train/sample-144613.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  gender\n",
       "0  data/cv-other-train/sample-069205.npy  female\n",
       "1  data/cv-valid-train/sample-063134.npy  female\n",
       "2  data/cv-other-train/sample-080873.npy  female\n",
       "3  data/cv-other-train/sample-105595.npy  female\n",
       "4  data/cv-valid-train/sample-144613.npy  female"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"balanced-all.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee13a620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66933</th>\n",
       "      <td>data/cv-valid-train/sample-171098.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66934</th>\n",
       "      <td>data/cv-other-train/sample-022864.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66935</th>\n",
       "      <td>data/cv-valid-train/sample-080933.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66936</th>\n",
       "      <td>data/cv-other-train/sample-012026.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66937</th>\n",
       "      <td>data/cv-other-train/sample-013841.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename gender\n",
       "66933  data/cv-valid-train/sample-171098.npy   male\n",
       "66934  data/cv-other-train/sample-022864.npy   male\n",
       "66935  data/cv-valid-train/sample-080933.npy   male\n",
       "66936  data/cv-other-train/sample-012026.npy   male\n",
       "66937  data/cv-other-train/sample-013841.npy   male"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9029aa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 66938\n",
      "total male sample: 33469\n",
      "total female sample: 33469\n"
     ]
    }
   ],
   "source": [
    "n_sample=len(df)\n",
    "n_male_sample=len(df[df['gender']=='male'])\n",
    "n_female_sample=len(df[df['gender']=='female'])\n",
    "print(\"total samples :\",n_sample)\n",
    "print(\"total male sample:\",n_male_sample)\n",
    "print(\"total female sample:\",n_female_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25614a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b752627",
   "metadata": {},
   "outputs": [],
   "source": [
    "lablel2int={\n",
    "    \"male\":1,\n",
    "    \"female\":0\n",
    "}\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    if os.path.isfile(\"results/features.npy\") and os.path.isfile(\"results/lables.npy\"):\n",
    "        X=np.load(\"results/features.npy\")\n",
    "        y=np.load(\"results/labels.npy\")\n",
    "        return X,y\n",
    "    df=pd.read_csv(\"balanced-all.csv\")\n",
    "    n_sample=len(df)\n",
    "    n_male_sample=len(df[df['gender']=='male'])\n",
    "    n_female_sample=len(df[df['gender']=='female'])\n",
    "    print(\"total samples :\",n_sample)\n",
    "    print(\"total male sample:\",n_male_sample)\n",
    "    print(\"total female sample:\",n_female_sample)\n",
    "    \n",
    "    X=np.zeros((n_sample, vector_length))\n",
    "    y=np.zeros((n_sample,1))\n",
    "    \n",
    "    for i,(filename,gender) in tqdm.tqdm(enumerate(zip(df['filename'],df['gender'])),\"Loding data\",total=n_sample):\n",
    "        features=np.load(filename)\n",
    "        X[i]=features\n",
    "        y[i]=lablel2int[gender]\n",
    "        \n",
    "    np.save(\"results/features\",X)\n",
    "    np.save(\"results/labels\",y)\n",
    "    return X,y\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67cd2f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 66938\n",
      "total male sample: 33469\n",
      "total female sample: 33469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loding data: 100%|███████████████████████████████████████████████████████████████| 66938/66938 [12:07<00:00, 91.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[8.33691835e-01, 1.84069288e+00, 7.21150815e-01, ...,\n",
       "         2.22568936e-03, 3.72419163e-04, 4.14350980e-05],\n",
       "        [3.44792323e-04, 3.78249679e-04, 7.02428690e-04, ...,\n",
       "         5.25400345e-08, 2.83253740e-08, 2.14376583e-09],\n",
       "        [1.90136401e-04, 2.43997492e-04, 4.60427953e-04, ...,\n",
       "         1.14822782e-04, 2.32082821e-05, 1.38619748e-06],\n",
       "        ...,\n",
       "        [1.16413343e-03, 9.43325169e-04, 1.52071775e-03, ...,\n",
       "         3.66348862e-09, 1.36717937e-09, 1.47780455e-10],\n",
       "        [6.36986643e-02, 1.34973586e-01, 2.42811322e+00, ...,\n",
       "         3.79042875e-04, 8.72957244e-05, 6.89193166e-06],\n",
       "        [1.45035911e+00, 1.17254317e+00, 2.72492021e-01, ...,\n",
       "         2.80375849e-03, 1.14007434e-03, 1.97923204e-04]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276bc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c91ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y,test_size=0.1,valid_size=0.1):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_size,random_state=7)\n",
    "    X_train,X_valid,y_train,y_valid=train_test_split(X_train,y_train,test_size=valid_size,random_state=7)\n",
    "    return{\n",
    "        \"X_train\":X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb6129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 66938\n",
      "total male sample: 33469\n",
      "total female sample: 33469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loding data: 100%|███████████████████████████████████████████████████████████████| 66938/66938 [15:09<00:00, 73.60it/s]\n"
     ]
    }
   ],
   "source": [
    "X,y=load_data()\n",
    "data=split_data(X,y,test_size=0.1,valid_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e2ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b608b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vector_length=128):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(256,input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",metrics=[\"accuracy\"],optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598f7f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 205825 (804.00 KB)\n",
      "Trainable params: 205825 (804.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1513d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "666fb4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "848/848 [==============================] - 15s 13ms/step - loss: 0.5495 - accuracy: 0.7745 - val_loss: 0.3645 - val_accuracy: 0.8576\n",
      "Epoch 2/100\n",
      "848/848 [==============================] - 11s 13ms/step - loss: 0.4077 - accuracy: 0.8390 - val_loss: 0.3389 - val_accuracy: 0.8657\n",
      "Epoch 3/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3784 - accuracy: 0.8523 - val_loss: 0.3061 - val_accuracy: 0.8825\n",
      "Epoch 4/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3606 - accuracy: 0.8602 - val_loss: 0.3045 - val_accuracy: 0.8815\n",
      "Epoch 5/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3452 - accuracy: 0.8690 - val_loss: 0.2921 - val_accuracy: 0.8915\n",
      "Epoch 6/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3306 - accuracy: 0.8740 - val_loss: 0.2864 - val_accuracy: 0.8903\n",
      "Epoch 7/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3283 - accuracy: 0.8768 - val_loss: 0.2758 - val_accuracy: 0.8949\n",
      "Epoch 8/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3214 - accuracy: 0.8795 - val_loss: 0.2716 - val_accuracy: 0.8948\n",
      "Epoch 9/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3143 - accuracy: 0.8819 - val_loss: 0.2569 - val_accuracy: 0.9037\n",
      "Epoch 10/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3026 - accuracy: 0.8854 - val_loss: 0.2569 - val_accuracy: 0.9057\n",
      "Epoch 11/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.3001 - accuracy: 0.8867 - val_loss: 0.2527 - val_accuracy: 0.9039\n",
      "Epoch 12/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2972 - accuracy: 0.8895 - val_loss: 0.2514 - val_accuracy: 0.9054\n",
      "Epoch 13/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2950 - accuracy: 0.8907 - val_loss: 0.2527 - val_accuracy: 0.9044\n",
      "Epoch 14/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2906 - accuracy: 0.8906 - val_loss: 0.2522 - val_accuracy: 0.9079\n",
      "Epoch 15/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2859 - accuracy: 0.8932 - val_loss: 0.2515 - val_accuracy: 0.9034\n",
      "Epoch 16/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2858 - accuracy: 0.8941 - val_loss: 0.2435 - val_accuracy: 0.9074\n",
      "Epoch 17/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2836 - accuracy: 0.8929 - val_loss: 0.2448 - val_accuracy: 0.9074\n",
      "Epoch 18/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2782 - accuracy: 0.8967 - val_loss: 0.2360 - val_accuracy: 0.9120\n",
      "Epoch 19/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2801 - accuracy: 0.8951 - val_loss: 0.2420 - val_accuracy: 0.9114\n",
      "Epoch 20/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2797 - accuracy: 0.8960 - val_loss: 0.2573 - val_accuracy: 0.9042\n",
      "Epoch 21/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2783 - accuracy: 0.8987 - val_loss: 0.2355 - val_accuracy: 0.9135\n",
      "Epoch 22/100\n",
      "848/848 [==============================] - 11s 12ms/step - loss: 0.2739 - accuracy: 0.8993 - val_loss: 0.2403 - val_accuracy: 0.9135\n",
      "Epoch 23/100\n",
      "848/848 [==============================] - 11s 12ms/step - loss: 0.2699 - accuracy: 0.9006 - val_loss: 0.2361 - val_accuracy: 0.9124\n",
      "Epoch 24/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2741 - accuracy: 0.9012 - val_loss: 0.2314 - val_accuracy: 0.9175\n",
      "Epoch 25/100\n",
      "848/848 [==============================] - 11s 12ms/step - loss: 0.2667 - accuracy: 0.8996 - val_loss: 0.2303 - val_accuracy: 0.9105\n",
      "Epoch 26/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2697 - accuracy: 0.9002 - val_loss: 0.2326 - val_accuracy: 0.9132\n",
      "Epoch 27/100\n",
      "848/848 [==============================] - 11s 12ms/step - loss: 0.2656 - accuracy: 0.9034 - val_loss: 0.2288 - val_accuracy: 0.9122\n",
      "Epoch 28/100\n",
      "848/848 [==============================] - 11s 12ms/step - loss: 0.2634 - accuracy: 0.9044 - val_loss: 0.2373 - val_accuracy: 0.9124\n",
      "Epoch 29/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2650 - accuracy: 0.9017 - val_loss: 0.2335 - val_accuracy: 0.9142\n",
      "Epoch 30/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2608 - accuracy: 0.9035 - val_loss: 0.2245 - val_accuracy: 0.9165\n",
      "Epoch 31/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2606 - accuracy: 0.9050 - val_loss: 0.2288 - val_accuracy: 0.9122\n",
      "Epoch 32/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2601 - accuracy: 0.9050 - val_loss: 0.2258 - val_accuracy: 0.9129\n",
      "Epoch 33/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2562 - accuracy: 0.9057 - val_loss: 0.2199 - val_accuracy: 0.9193\n",
      "Epoch 34/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2659 - accuracy: 0.9055 - val_loss: 0.2268 - val_accuracy: 0.9175\n",
      "Epoch 35/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2541 - accuracy: 0.9077 - val_loss: 0.2248 - val_accuracy: 0.9149\n",
      "Epoch 36/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2563 - accuracy: 0.9060 - val_loss: 0.2237 - val_accuracy: 0.9197\n",
      "Epoch 37/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2552 - accuracy: 0.9069 - val_loss: 0.2297 - val_accuracy: 0.9167\n",
      "Epoch 38/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2565 - accuracy: 0.9058 - val_loss: 0.2203 - val_accuracy: 0.9187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2157802a520>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir=\"logs\")\n",
    "early_stopping=EarlyStopping(mode=\"min\",patience=5,restore_best_weights=True)\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "epochs=100\n",
    "\n",
    "model.fit(data[\"X_train\"],data[\"y_train\"],epochs=epochs,batch_size=batch_size,validation_data=(data[\"X_valid\"],data[\"y_valid\"]),\n",
    "         callbacks=[tensorboard,early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e71c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riya\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model in the native Keras format\n",
    "model.save(\"results/model.keras\")\n",
    "model.save(\"results/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d6d1478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model using 6694 samples...\n",
      "210/210 [==============================] - 1s 5ms/step - loss: 0.2227 - accuracy: 0.9183\n",
      "Loss:0.2227\n",
      "Accuracy: 91.83%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss,accuracy=model.evaluate(data[\"X_test\"],data[\"y_test\"],verbose=1)\n",
    "\n",
    "print(f\"Loss:{loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26242cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librosa is installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import librosa\n",
    "    print(\"librosa is installed.\")\n",
    "except ImportError:\n",
    "    print(\"librosa is not installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9af65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    \n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d95eda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f36be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Speak into the microphone...\n",
      "Recording completed.\n",
      "Saved recorded voice to 'recorded_voice.wav'.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 5\n",
    "OUTPUT_FILE = \"recorded_voice.wav\"\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open the audio stream\n",
    "stream = audio.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Recording started. Speak into the microphone...\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "# Record audio for the specified duration\n",
    "for i in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"Recording completed.\")\n",
    "\n",
    "# Stop and close the audio stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Save the recorded audio to a WAV file\n",
    "with wave.open(OUTPUT_FILE, \"wb\") as wf:\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "print(f\"Saved recorded voice to '{OUTPUT_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5292c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 281ms/step\n",
      "Result: male\n",
      "Probabilities:     Male: 88.72%    Female: 11.28%\n"
     ]
    }
   ],
   "source": [
    "file_name=\"C:/Users/Riya/OneDrive/Desktop/voice detection/gender-recognition-by-voice/recorded_voice.wav\"\n",
    "features = extract_feature(file_name, mel=True).reshape(1, -1)\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "\n",
    "# show the result!\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities:     Male: {male_prob * 100:.2f}%    Female: {female_prob * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cde8891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "from PIL import ImageTk,Image\n",
    "root=Tk()\n",
    "\n",
    "\n",
    "root.title(\"Voice-based gender detection\")\n",
    "root.geometry(\"600x400\")\n",
    "root.minsize(300,300)\n",
    "root.maxsize(700,700)\n",
    "\n",
    "root.iconbitmap(\"D:\\\\tkinter\\\\logo.ico\")\n",
    "root.configure(bg=\"lightblue\")\n",
    "\n",
    "header=Label(root,text=\"Voice based gender identification\",bg=\"lightblue\",\n",
    "            foreground=\"black\",font=(\"Arial\",24,\"bold\"))\n",
    "header.pack(pady=10)\n",
    "# root.wm_attributes('-transparentcolor',\"purple\")\n",
    "\n",
    "def record():\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    RECORD_SECONDS = 5\n",
    "    OUTPUT_FILE = \"recorded_voice.wav\"\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open the audio stream\n",
    "    stream = audio.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording started. Speak into the microphone...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    # Record audio for the specified duration\n",
    "    for i in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording completed.\")\n",
    "\n",
    "    # Stop and close the audio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded audio to a WAV file\n",
    "    with wave.open(OUTPUT_FILE, \"wb\") as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "    print(f\"Saved recorded voice to '{OUTPUT_FILE}'.\")\n",
    "button1=Button(text=\"Record voice\",bg=\"lightblue\",activebackground=\"blue\",font=(\"Arial\",18,\"bold\"),\n",
    "      borderwidth=3,command=record)\n",
    "button1.pack(pady=30)\n",
    "predict_label=Label(root,text=\"\",bg=\"lightblue\",font=(\"Arial\",15,\"bold\"))\n",
    "predict_label.pack()\n",
    "\n",
    "def predict_gender():\n",
    "    file_name=\"C:/Users/Riya/OneDrive/Desktop/voice detection/gender-recognition-by-voice/recorded_voice.wav\"\n",
    "    features = extract_feature(file_name, mel=True).reshape(1, -1)\n",
    "    male_prob = model.predict(features)[0][0]\n",
    "    female_prob = 1 - male_prob\n",
    "    gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "\n",
    "    # show the result!\n",
    "    print(\"Result:\", gender)\n",
    "    print(f\"Probabilities:     Male: {male_prob * 100:.2f}%    Female: {female_prob * 100:.2f}%\")\n",
    "    predict_label.config(text=\"Predicted Gender :\"+gender)\n",
    "\n",
    "button2=Button(text=\"Gender\",bg=\"lightblue\",activebackground=\"blue\",font=(\"Arial\",18,\"bold\"),\n",
    "      borderwidth=3,command=predict_gender)\n",
    "button2.pack(pady=25)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c2be516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02ff66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645795bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
